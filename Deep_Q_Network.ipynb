{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Q Network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4BxIL4sYUPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg- dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "!apt-get install xvfb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXLrHVg8Yopm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym\n",
        "!pip install gym[atari]\n",
        "!pip install gym-retro\n",
        "!pip install tqdm retrowrapper gym-retro\n",
        "!pip install -U git+git://github.com/frenchie4111/dumbrain.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVZ1DlYyZmWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "!unzip ROMS.zip\n",
        "!python -m retro.import"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHWZ92tuYlBg",
        "colab_type": "code",
        "outputId": "0d1edc7a-749a-4c30-a4fb-58948d3ed065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1005'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1005'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlpjmYnGYDvS",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "75f70ba5-68a6-4384-da5b-cf825e7db890",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2340
        }
      },
      "source": [
        "import tensorflow as tf      # Deep Learning library\n",
        "import numpy as np           # Handle matrices\n",
        "import retro                 # Retro Environment\n",
        "from skimage import transform # Help us to preprocess the frames\n",
        "from skimage.color import rgb2gray # Help us to gray our frames\n",
        "import matplotlib.pyplot as plt # Display graphs\n",
        "from collections import deque# Ordered collection with ends\n",
        "import random\n",
        "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import pprint\n",
        "import psutil\n",
        "import gym\n",
        "process = psutil.Process(os.getpid())\n",
        "with tf.Session() as session:\n",
        "  devices = session.list_devices()\n",
        "    \n",
        "print('devices:')\n",
        "pprint.pprint(devices)\n",
        "\n",
        "env = gym.make('PongDeterministic-v4')\n",
        "#env = retro.make('SpaceInvaders-Atari2600')\n",
        "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
        "print(possible_actions)\n",
        "    \n",
        "# Model Hyperparameters\n",
        "state_size = [110,84,4] #4 frames of 110x84\n",
        "action_size = env.action_space.n\n",
        "learning_rate = 0.00025\n",
        "\n",
        "# Training Hyperparameters\n",
        "total_episodes = 210\n",
        "total_test_episodes = 2\n",
        "max_steps = 50000\n",
        "batch_size = 64\n",
        "\n",
        "# Epsilon values\n",
        "explore_start = 1.0\n",
        "explore_stop = 0.01\n",
        "decay_rate = 0.00001\n",
        "\n",
        "# Discount\n",
        "gamma = 0.9\n",
        "\n",
        "# Memory Hyperparameters\n",
        "pretrain_length = batch_size #initial memory size\n",
        "memory_size = 30000 #1000000\n",
        "\n",
        "# Preprocessing Hyperparameters\n",
        "stack_size = 4\n",
        "\n",
        "# Misc. Hyperparameters\n",
        "training = True\n",
        "episode_render = False\n",
        "restart = True\n",
        "\n",
        "# Deep-Q Network\n",
        "class DQNetwork():\n",
        "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        with tf.variable_scope(name):\n",
        "            self.inputs_ = tf.placeholder(tf.float32,[None,*self.state_size], name=\"inputs\")\n",
        "            self.actions_ = tf.placeholder(tf.float32, [None,self.action_size], name=\"actions_\")\n",
        "            \n",
        "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
        "            \n",
        "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
        "            filters=32,\n",
        "            kernel_size = [8,8],\n",
        "            strides=[4,4],\n",
        "            padding=\"VALID\",\n",
        "            kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "            name=\"conv1\")\n",
        "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\")\n",
        "            \n",
        "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
        "            filters=64,\n",
        "            kernel_size=[4,4],\n",
        "            strides=[2,2],\n",
        "            padding=\"VALID\",\n",
        "            kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "            name=\"conv2\")\n",
        "            self.conv2_out = tf.nn.elu(self.conv2,name=\"conv2_out\")\n",
        "            \n",
        "            self.conv3 = tf.layers.conv2d(inputs=self.conv2_out,\n",
        "            filters=64,\n",
        "            kernel_size=[3,3],\n",
        "            strides=[2,2],\n",
        "            padding=\"VALID\",\n",
        "            kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "            name=\"conv3\")\n",
        "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\")\n",
        "            \n",
        "            self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
        "            \n",
        "            self.fc = tf.layers.dense(inputs = self.flatten,\n",
        "            units=512,\n",
        "            activation=tf.nn.elu,\n",
        "            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "            name=\"fc1\")\n",
        "            \n",
        "            self.output = tf.layers.dense(inputs=self.fc,\n",
        "            units=self.action_size,\n",
        "            activation=None)\n",
        "            \n",
        "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_)) # list of q-values per possible action\n",
        "            \n",
        "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
        "            \n",
        "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
        "\n",
        "class Memory():\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "    \n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        buffer_size = len(self.buffer)\n",
        "        indices = np.random.choice(np.arange(buffer_size), size=batch_size, replace=False)\n",
        "        \n",
        "        return [self.buffer[i] for i in indices]\n",
        "\n",
        "\n",
        "\n",
        "# Preproccessing functions\n",
        "\n",
        "#Grayscale and downscale\n",
        "def preprocess_frame(frame):\n",
        "    gray = rgb2gray(frame)\n",
        "    \n",
        "    #cropped_frame = gray[8:-12,4:-12]\n",
        "    cropped_frame = gray\n",
        "    \n",
        "    normalized_frame = cropped_frame/255.0\n",
        "    \n",
        "    preprocessed_frame = transform.resize(cropped_frame, [110,84])\n",
        "    return preprocessed_frame\n",
        "\n",
        "\n",
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "    frame = preprocess_frame(state)\n",
        "    if is_new_episode:\n",
        "        stacked_frames = deque([], maxlen=stack_size)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        \n",
        "        \n",
        "        stacked_state = np.stack(stacked_frames,axis=2)\n",
        "    else:\n",
        "        #automatically removed oldest frame\n",
        "        stacked_frames.append(frame)\n",
        "        \n",
        "        stacked_state = np.stack(stacked_frames,axis=2)\n",
        "    \n",
        "    return stacked_state,stacked_frames\n",
        "\n",
        "def initialize_memory():\n",
        "    stacked_frames = deque([np.zeros((110,84),dtype=np.int) for i in range(stack_size)], maxlen=stack_size)\n",
        "    memory = Memory(max_size=memory_size)\n",
        "    for i in range(pretrain_length):\n",
        "        if i == 0:\n",
        "            state = env.reset()\n",
        "            state, stacked_frames = stack_frames(stacked_frames,state,True)\n",
        "        \n",
        "        randChoice = random.randint(0, len(possible_actions)-1)\n",
        "        action = possible_actions[randChoice]\n",
        "        \n",
        "        next_state,reward,done,_ = env.step(randChoice)\n",
        "        \n",
        "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "        \n",
        "        if done:\n",
        "            next_state = np.zeros(state.shape)\n",
        "            memory.add((state,action,reward,next_state,done))\n",
        "            state = env.reset()\n",
        "            state,stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "        else:\n",
        "            memory.add((state,action,reward,next_state,done))\n",
        "            state=next_state\n",
        "    return memory, stacked_frames\n",
        "        \n",
        "def get_action(epsilon_initial, epsilon_final, decay_rate, decay_step, state, network, sess):\n",
        "    # decay epsilon\n",
        "    new_eps = epsilon_final + (epsilon_initial - epsilon_final) * np.exp(-decay_rate * decay_step)\n",
        "    rand_value = random.uniform(0, 1)\n",
        "    \n",
        "    if rand_value < new_eps:\n",
        "        action_choice = possible_actions[random.randint(0, len(possible_actions) - 1)]\n",
        "    else:\n",
        "        q_values = sess.run(network.output, feed_dict={network.inputs_: state.reshape((1, *state.shape))})\n",
        "        action_index = np.argmax(q_values)\n",
        "        action_choice = possible_actions[action_index]\n",
        "    \n",
        "    return action_choice, new_eps\n",
        "    \n",
        "    \n",
        "def main():\n",
        "    print(\"Running\")\n",
        "    tf.reset_default_graph()\n",
        "    network = DQNetwork(state_size, action_size, learning_rate)\n",
        "    \n",
        "    writer = tf.summary.FileWriter(\"./tensorboard/dqn/1\")\n",
        "    tf.summary.scalar(\"Loss\", network.loss)\n",
        "    write_op = tf.summary.merge_all()\n",
        "    \n",
        "    \n",
        "    memory, stacked_frames = initialize_memory()\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    if training:\n",
        "        rewards_list = []\n",
        "        config = tf.ConfigProto()\n",
        "        config.gpu_options.allow_growth = True\n",
        "        with tf.Session(config = config) as sess:\n",
        "            #initialize weights\n",
        "            if restart:\n",
        "                sess.run(tf.global_variables_initializer())\n",
        "            else:\n",
        "                saver.restore(sess, \"./models/model.ckpt\")\n",
        "\n",
        "            #initialize decay\n",
        "            decay_step = 0\n",
        "            \n",
        "            #initialize loss\n",
        "            loss = 0\n",
        "            print(\"Network initialized.\")\n",
        "            for episode in range(total_episodes):\n",
        "                #initialize environment\n",
        "                step = 0\n",
        "                state = env.reset()\n",
        "                \n",
        "                episode_rewards = []\n",
        "                \n",
        "                state,stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "                \n",
        "                while (step < max_steps):\n",
        "                    # play game\n",
        "                    step += 1\n",
        "                    decay_step += 1\n",
        "                    action_choice, eps = get_action(explore_start, explore_stop, decay_rate, decay_step, state, network, sess)\n",
        "                    action_index = 0\n",
        "                    for i in range(len(action_choice)):\n",
        "                        if action_choice[i] == 1:\n",
        "                            action_index = i\n",
        "                            \n",
        "                    new_state, reward, done, _ = env.step(action_index)\n",
        "                    \n",
        "                    if episode_render:\n",
        "                        env.render()\n",
        "                        \n",
        "                    episode_rewards.append(reward)\n",
        "                    \n",
        "                    if done:\n",
        "                        new_state = np.zeros((110,84), dtype=int)\n",
        "                        new_state, stacked_frames = stack_frames(stacked_frames, new_state, False)\n",
        "                        \n",
        "                        step = max_steps\n",
        "                        \n",
        "                        total_reward = np.sum(episode_rewards)\n",
        "                        \n",
        "                        print('Episode: {}'.format(episode),\n",
        "                                  'Total reward: {}'.format(total_reward),\n",
        "                                  'Explore P: {:.4f}'.format(eps),\n",
        "                                'Training Loss {:.4f}'.format(loss),\n",
        "                                'Decay Step: {}'.format(decay_step),\n",
        "                                'RAM: {}'.format(process.memory_info().rss),\n",
        "                                'Memory Size: {}'.format(len(memory.buffer)))\n",
        "\n",
        "                        rewards_list.append((episode, total_reward))\n",
        "                        \n",
        "                        new_transition = (state, action_choice, reward, new_state, done)\n",
        "                        memory.add(new_transition)\n",
        "                        \n",
        "                        \n",
        "                    else:\n",
        "                        new_state, stacked_frames = stack_frames(stacked_frames, new_state, False)\n",
        "                        \n",
        "                        new_transition = (state, action_choice, reward, new_state, done)\n",
        "                        memory.add(new_transition)\n",
        "                        \n",
        "                        state = new_state\n",
        "                    \n",
        "                    # train network\n",
        "                    \n",
        "                    sample_transitions = memory.sample(batch_size) #shape: (64, 5)\n",
        "                    sample_states = np.array([transition[0] for transition in sample_transitions])\n",
        "                    sample_actions = np.array([transition[1] for transition in sample_transitions])\n",
        "                    sample_rewards = np.array([transition[2] for transition in sample_transitions])\n",
        "                    sample_dones = np.array([transition[4] for transition in sample_transitions])\n",
        "                    sample_next_states = np.array([transition[3] for transition in sample_transitions])\n",
        "                    \n",
        "                    sample_target_qs = []\n",
        "                    \n",
        "                    sample_q_next_states = sess.run(network.output, feed_dict = {network.inputs_: sample_next_states})\n",
        "                    \n",
        "                    for i in range(len(sample_transitions)):\n",
        "                        if sample_dones[i]:\n",
        "                            sample_target_qs.append(sample_rewards[i])\n",
        "                        else:\n",
        "                            sample_target_qs.append(sample_rewards[i] + gamma*np.max(sample_q_next_states[i]))\n",
        "                            \n",
        "                    # optimize weights\n",
        "                    \n",
        "                    loss, _ = sess.run([network.loss, network.optimizer], feed_dict={network.inputs_: sample_states, network.actions_: sample_actions, network.target_Q: sample_target_qs})\n",
        "                    \n",
        "                    summary = sess.run(write_op, feed_dict={network.inputs_: sample_states, network.actions_: sample_actions, network.target_Q: sample_target_qs})\n",
        "                    \n",
        "                    writer.add_summary(summary, episode)\n",
        "                    writer.flush()\n",
        "                \n",
        "                if episode % 5 == 0:\n",
        "                    save_path = saver.save(sess, \"./models/model.ckpt\")\n",
        "                    print(\"Model Saved\")\n",
        "    \n",
        "    # Test model\n",
        "    with tf.Session() as sess:\n",
        "        total_test_rewards = []\n",
        "        \n",
        "        saver.restore(sess, \"./models/model.ckpt\")\n",
        "        \n",
        "        for episode in range(total_test_episodes):\n",
        "            total_rewards = 0\n",
        "            \n",
        "            state = env.reset()\n",
        "            state,stacked_frames = stack_frames(stacked_frames,state,True)\n",
        "            \n",
        "            print(\"Beginning episode \", episode)\n",
        "            \n",
        "            while True:\n",
        "                \n",
        "                q_values = sess.run(network.output, feed_dict={network.inputs_: state.reshape((1, *state.shape))})\n",
        "                action_index = np.argmax(q_values)\n",
        "                action_choice = possible_actions[action_index]\n",
        "                                \n",
        "                next_state, reward, done, _ = env.step(action_index) #action_choice\n",
        "                total_rewards += reward\n",
        "                \n",
        "                env.render()\n",
        "                \n",
        "                if done:\n",
        "                    print(\"Score: \", total_rewards)\n",
        "                    total_test_rewards.append(total_rewards)\n",
        "                    break\n",
        "                \n",
        "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "                state = next_state\n",
        "            \n",
        "    env.close()\n",
        "    \n",
        "main()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "devices:\n",
            "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 6647090966268779493),\n",
            " _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7668213009845780850),\n",
            " _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 2110971038642368049),\n",
            " _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 14800692839, 4952065922317766284)]\n",
            "[[1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0]\n",
            " [0 0 1 0 0 0]\n",
            " [0 0 0 1 0 0]\n",
            " [0 0 0 0 1 0]\n",
            " [0 0 0 0 0 1]]\n",
            "Running\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-1-a0695a3fd869>:77: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-1-a0695a3fd869>:104: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "Network initialized.\n",
            "Episode: 0 Total reward: 105.0 Explore P: 0.9948 Training Loss 7.1476 Decay Step: 527 RAM: 2702311424 Memory Size: 590\n",
            "Model Saved\n",
            "Episode: 1 Total reward: 235.0 Explore P: 0.9855 Training Loss 1.8362 Decay Step: 1475 RAM: 2999472128 Memory Size: 1538\n",
            "Episode: 2 Total reward: 325.0 Explore P: 0.9733 Training Loss 19.1405 Decay Step: 2738 RAM: 3378974720 Memory Size: 2801\n",
            "Episode: 3 Total reward: 50.0 Explore P: 0.9695 Training Loss 0.0416 Decay Step: 3131 RAM: 3511705600 Memory Size: 3194\n",
            "Episode: 4 Total reward: 120.0 Explore P: 0.9607 Training Loss 15.6830 Decay Step: 4046 RAM: 3796111360 Memory Size: 4109\n",
            "Episode: 5 Total reward: 125.0 Explore P: 0.9507 Training Loss 0.0007 Decay Step: 5103 RAM: 4118360064 Memory Size: 5166\n",
            "Model Saved\n",
            "Episode: 6 Total reward: 125.0 Explore P: 0.9438 Training Loss 9.8231 Decay Step: 5840 RAM: 4345982976 Memory Size: 5903\n",
            "Episode: 7 Total reward: 210.0 Explore P: 0.9362 Training Loss 0.3041 Decay Step: 6665 RAM: 4592619520 Memory Size: 6728\n",
            "Episode: 8 Total reward: 355.0 Explore P: 0.9290 Training Loss 0.2016 Decay Step: 7444 RAM: 4838797312 Memory Size: 7507\n",
            "Episode: 9 Total reward: 110.0 Explore P: 0.9212 Training Loss 0.1126 Decay Step: 8296 RAM: 5085712384 Memory Size: 8359\n",
            "Episode: 10 Total reward: 90.0 Explore P: 0.9161 Training Loss 0.1186 Decay Step: 8859 RAM: 5275217920 Memory Size: 8922\n",
            "Model Saved\n",
            "Episode: 11 Total reward: 50.0 Explore P: 0.9110 Training Loss 6.1708 Decay Step: 9416 RAM: 5445951488 Memory Size: 9479\n",
            "Episode: 12 Total reward: 230.0 Explore P: 0.9026 Training Loss 0.0011 Decay Step: 10358 RAM: 5730185216 Memory Size: 10421\n",
            "Episode: 13 Total reward: 80.0 Explore P: 0.8992 Training Loss 0.1431 Decay Step: 10742 RAM: 5843996672 Memory Size: 10805\n",
            "Episode: 14 Total reward: 110.0 Explore P: 0.8955 Training Loss 0.1742 Decay Step: 11150 RAM: 5976616960 Memory Size: 11213\n",
            "Episode: 15 Total reward: 135.0 Explore P: 0.8900 Training Loss 1.5402 Decay Step: 11780 RAM: 6166663168 Memory Size: 11843\n",
            "Model Saved\n",
            "Episode: 16 Total reward: 210.0 Explore P: 0.8828 Training Loss 0.0001 Decay Step: 12597 RAM: 6412939264 Memory Size: 12660\n",
            "Episode: 17 Total reward: 225.0 Explore P: 0.8730 Training Loss 0.5403 Decay Step: 13730 RAM: 6754217984 Memory Size: 13793\n",
            "Episode: 18 Total reward: 155.0 Explore P: 0.8662 Training Loss 0.0850 Decay Step: 14523 RAM: 7000670208 Memory Size: 14586\n",
            "Episode: 19 Total reward: 155.0 Explore P: 0.8601 Training Loss 9.5222 Decay Step: 15232 RAM: 7228325888 Memory Size: 15295\n",
            "Episode: 20 Total reward: 210.0 Explore P: 0.8534 Training Loss 0.0148 Decay Step: 16023 RAM: 7475060736 Memory Size: 16086\n",
            "Model Saved\n",
            "Episode: 21 Total reward: 35.0 Explore P: 0.8502 Training Loss 3.8552 Decay Step: 16412 RAM: 7588855808 Memory Size: 16475\n",
            "Episode: 22 Total reward: 60.0 Explore P: 0.8467 Training Loss 0.2411 Decay Step: 16823 RAM: 7721541632 Memory Size: 16886\n",
            "Episode: 23 Total reward: 240.0 Explore P: 0.8402 Training Loss 17.2115 Decay Step: 17600 RAM: 7967817728 Memory Size: 17663\n",
            "Episode: 24 Total reward: 135.0 Explore P: 0.8345 Training Loss 3.3683 Decay Step: 18288 RAM: 8176439296 Memory Size: 18351\n",
            "Episode: 25 Total reward: 70.0 Explore P: 0.8300 Training Loss 0.3914 Decay Step: 18835 RAM: 8347193344 Memory Size: 18898\n",
            "Model Saved\n",
            "Episode: 26 Total reward: 225.0 Explore P: 0.8225 Training Loss 9.6561 Decay Step: 19753 RAM: 8631508992 Memory Size: 19816\n",
            "Episode: 27 Total reward: 210.0 Explore P: 0.8159 Training Loss 0.0500 Decay Step: 20570 RAM: 8878366720 Memory Size: 20633\n",
            "Episode: 28 Total reward: 110.0 Explore P: 0.8103 Training Loss 22.4865 Decay Step: 21267 RAM: 9105715200 Memory Size: 21330\n",
            "Episode: 29 Total reward: 195.0 Explore P: 0.8037 Training Loss 7.5585 Decay Step: 22105 RAM: 9352302592 Memory Size: 22168\n",
            "Episode: 30 Total reward: 295.0 Explore P: 0.7969 Training Loss 13.9180 Decay Step: 22959 RAM: 9636847616 Memory Size: 23022\n",
            "Model Saved\n",
            "Episode: 31 Total reward: 465.0 Explore P: 0.7894 Training Loss 1.5381 Decay Step: 23922 RAM: 9940099072 Memory Size: 23985\n",
            "Episode: 32 Total reward: 105.0 Explore P: 0.7846 Training Loss 43.8069 Decay Step: 24540 RAM: 10129805312 Memory Size: 24603\n",
            "Episode: 33 Total reward: 390.0 Explore P: 0.7740 Training Loss 3.8328 Decay Step: 25913 RAM: 10546933760 Memory Size: 25976\n",
            "Episode: 34 Total reward: 210.0 Explore P: 0.7679 Training Loss 3.4793 Decay Step: 26716 RAM: 10812526592 Memory Size: 26779\n",
            "Episode: 35 Total reward: 140.0 Explore P: 0.7619 Training Loss 0.1106 Decay Step: 27508 RAM: 11058872320 Memory Size: 27571\n",
            "Model Saved\n",
            "Episode: 36 Total reward: 155.0 Explore P: 0.7561 Training Loss 0.0087 Decay Step: 28287 RAM: 11305418752 Memory Size: 28350\n",
            "Episode: 37 Total reward: 120.0 Explore P: 0.7511 Training Loss 0.1100 Decay Step: 28952 RAM: 11514118144 Memory Size: 29015\n",
            "Episode: 38 Total reward: 155.0 Explore P: 0.7433 Training Loss 0.1242 Decay Step: 30021 RAM: 11802103808 Memory Size: 30000\n",
            "Episode: 39 Total reward: 110.0 Explore P: 0.7386 Training Loss 7.6132 Decay Step: 30658 RAM: 11802673152 Memory Size: 30000\n",
            "Episode: 40 Total reward: 180.0 Explore P: 0.7340 Training Loss 1.5467 Decay Step: 31294 RAM: 11798564864 Memory Size: 30000\n",
            "Model Saved\n",
            "Episode: 41 Total reward: 350.0 Explore P: 0.7275 Training Loss 0.4237 Decay Step: 32199 RAM: 11799101440 Memory Size: 30000\n",
            "Episode: 42 Total reward: 50.0 Explore P: 0.7246 Training Loss 0.0253 Decay Step: 32598 RAM: 11813851136 Memory Size: 30000\n",
            "Episode: 43 Total reward: 35.0 Explore P: 0.7211 Training Loss 3.4601 Decay Step: 33090 RAM: 11813851136 Memory Size: 30000\n",
            "Episode: 44 Total reward: 30.0 Explore P: 0.7178 Training Loss 3.4625 Decay Step: 33556 RAM: 11813851136 Memory Size: 30000\n",
            "Episode: 45 Total reward: 175.0 Explore P: 0.7129 Training Loss 0.0165 Decay Step: 34248 RAM: 11813851136 Memory Size: 30000\n",
            "Model Saved\n",
            "Episode: 46 Total reward: 65.0 Explore P: 0.7101 Training Loss 3.4745 Decay Step: 34649 RAM: 11814117376 Memory Size: 30000\n",
            "Episode: 47 Total reward: 235.0 Explore P: 0.7042 Training Loss 9.6279 Decay Step: 35498 RAM: 11814117376 Memory Size: 30000\n",
            "Episode: 48 Total reward: 90.0 Explore P: 0.7013 Training Loss 0.4292 Decay Step: 35906 RAM: 11814117376 Memory Size: 30000\n",
            "Episode: 49 Total reward: 105.0 Explore P: 0.6985 Training Loss 6.1822 Decay Step: 36317 RAM: 11814117376 Memory Size: 30000\n",
            "Episode: 50 Total reward: 255.0 Explore P: 0.6940 Training Loss 14.3133 Decay Step: 36972 RAM: 11814117376 Memory Size: 30000\n",
            "Model Saved\n",
            "Episode: 51 Total reward: 110.0 Explore P: 0.6893 Training Loss 0.7616 Decay Step: 37665 RAM: 11814117376 Memory Size: 30000\n",
            "Episode: 52 Total reward: 155.0 Explore P: 0.6846 Training Loss 27.7398 Decay Step: 38356 RAM: 11814117376 Memory Size: 30000\n",
            "Episode: 53 Total reward: 100.0 Explore P: 0.6808 Training Loss 0.0142 Decay Step: 38921 RAM: 11814117376 Memory Size: 30000\n",
            "Episode: 54 Total reward: 215.0 Explore P: 0.6753 Training Loss 13.9315 Decay Step: 39751 RAM: 11814117376 Memory Size: 30000\n",
            "Episode: 55 Total reward: 505.0 Explore P: 0.6679 Training Loss 0.4304 Decay Step: 40864 RAM: 11813904384 Memory Size: 30000\n",
            "Model Saved\n",
            "Episode: 56 Total reward: 55.0 Explore P: 0.6652 Training Loss 3.4698 Decay Step: 41282 RAM: 11813904384 Memory Size: 30000\n",
            "Episode: 57 Total reward: 10.0 Explore P: 0.6619 Training Loss 0.3208 Decay Step: 41779 RAM: 11813904384 Memory Size: 30000\n",
            "Episode: 58 Total reward: 80.0 Explore P: 0.6589 Training Loss 3.0275 Decay Step: 42243 RAM: 11813904384 Memory Size: 30000\n",
            "Episode: 59 Total reward: 260.0 Explore P: 0.6538 Training Loss 3.8425 Decay Step: 43036 RAM: 11813904384 Memory Size: 30000\n",
            "Episode: 60 Total reward: 150.0 Explore P: 0.6502 Training Loss 1.5369 Decay Step: 43599 RAM: 11813904384 Memory Size: 30000\n",
            "Model Saved\n",
            "Episode: 61 Total reward: 85.0 Explore P: 0.6470 Training Loss 0.0562 Decay Step: 44086 RAM: 11813904384 Memory Size: 30000\n",
            "Episode: 62 Total reward: 140.0 Explore P: 0.6433 Training Loss 0.0416 Decay Step: 44670 RAM: 11813904384 Memory Size: 30000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a0695a3fd869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-a0695a3fd869>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    299\u001b[0m                     \u001b[0msample_target_qs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m                     \u001b[0msample_q_next_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample_next_states\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_transitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOWRnr37e2CZ",
        "colab_type": "text"
      },
      "source": [
        "Train model here and download it back to computer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEzES6eUE6Ml",
        "colab_type": "code",
        "outputId": "26039429-fb9b-40d4-88a8-715b3560278b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "!zip -r models.zip models/\n",
        "from google.colab import files\n",
        "files.download(\"./models.zip\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: models/ (stored 0%)\n",
            "updating: models/model.ckpt.data-00000-of-00001 (deflated 9%)\n",
            "updating: models/checkpoint (deflated 42%)\n",
            "updating: models/model.ckpt.meta (deflated 89%)\n",
            "updating: models/model.ckpt.index (deflated 49%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}