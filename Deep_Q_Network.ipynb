{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Q Network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m1234d/rl-projects/blob/master/Deep_Q_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4BxIL4sYUPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg- dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "!apt-get install xvfb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXLrHVg8Yopm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym\n",
        "!pip install gym[atari]\n",
        "!pip install gym-retro\n",
        "!pip install tqdm retrowrapper gym-retro\n",
        "!pip install -U git+git://github.com/frenchie4111/dumbrain.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVZ1DlYyZmWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "!python -m retro.import"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHWZ92tuYlBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlpjmYnGYDvS",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "import tensorflow as tf      # Deep Learning library\n",
        "import numpy as np           # Handle matrices\n",
        "import retro                 # Retro Environment\n",
        "from skimage import transform # Help us to preprocess the frames\n",
        "from skimage.color import rgb2gray # Help us to gray our frames\n",
        "import matplotlib.pyplot as plt # Display graphs\n",
        "from collections import deque# Ordered collection with ends\n",
        "import random\n",
        "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "env = retro.make(game='SpaceInvaders-Atari2600')\n",
        "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())\n",
        "print(possible_actions)\n",
        "    \n",
        "# Model Hyperparameters\n",
        "state_size = [110,84,4] #4 frames of 110x84\n",
        "action_size = env.action_space.n\n",
        "learning_rate = 0.00025\n",
        "\n",
        "# Training Hyperparameters\n",
        "total_episodes = 50\n",
        "max_steps = 50000\n",
        "batch_size = 64\n",
        "\n",
        "# Epsilon values\n",
        "explore_start = 1.0\n",
        "explore_stop = 0.01\n",
        "decay_rate = 0.00001\n",
        "\n",
        "# Discount\n",
        "gamma = 0.9\n",
        "\n",
        "# Memory Hyperparameters\n",
        "pretrain_length = batch_size #initial memory size\n",
        "memory_size = 1000000\n",
        "\n",
        "# Preprocessing Hyperparameters\n",
        "stack_size = 4\n",
        "\n",
        "# Misc. Hyperparameters\n",
        "training = True\n",
        "episode_render = False\n",
        "\n",
        "# Deep-Q Network\n",
        "class DQNetwork():\n",
        "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        with tf.variable_scope(name):\n",
        "            self.inputs_ = tf.placeholder(tf.float32,[None,*self.state_size], name=\"inputs\")\n",
        "            self.actions_ = tf.placeholder(tf.float32, [None,self.action_size], name=\"actions_\")\n",
        "            \n",
        "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
        "            \n",
        "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
        "            filters=32,\n",
        "            kernel_size = [8,8],\n",
        "            strides=[4,4],\n",
        "            padding=\"VALID\",\n",
        "            kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "            name=\"conv1\")\n",
        "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\")\n",
        "            \n",
        "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
        "            filters=64,\n",
        "            kernel_size=[4,4],\n",
        "            strides=[2,2],\n",
        "            padding=\"VALID\",\n",
        "            kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "            name=\"conv2\")\n",
        "            self.conv2_out = tf.nn.elu(self.conv2,name=\"conv2_out\")\n",
        "            \n",
        "            self.conv3 = tf.layers.conv2d(inputs=self.conv2_out,\n",
        "            filters=64,\n",
        "            kernel_size=[3,3],\n",
        "            strides=[2,2],\n",
        "            padding=\"VALID\",\n",
        "            kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "            name=\"conv3\")\n",
        "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\")\n",
        "            \n",
        "            self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
        "            \n",
        "            self.fc = tf.layers.dense(inputs = self.flatten,\n",
        "            units=512,\n",
        "            activation=tf.nn.elu,\n",
        "            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "            name=\"fc1\")\n",
        "            \n",
        "            self.output = tf.layers.dense(inputs=self.fc,\n",
        "            units=self.action_size,\n",
        "            activation=None)\n",
        "            \n",
        "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_)) # list of q-values per possible action\n",
        "            \n",
        "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
        "            \n",
        "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
        "\n",
        "class Memory():\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "    \n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        buffer_size = len(self.buffer)\n",
        "        indices = np.random.choice(np.arange(buffer_size), size=batch_size, replace=False)\n",
        "        \n",
        "        return [self.buffer[i] for i in indices]\n",
        "\n",
        "\n",
        "\n",
        "# Preproccessing functions\n",
        "\n",
        "#Grayscale and downscale\n",
        "def preprocess_frame(frame):\n",
        "    gray = rgb2gray(frame)\n",
        "    \n",
        "    cropped_frame = gray[8:-12,4:-12]\n",
        "    \n",
        "    normalized_frame = cropped_frame/255.0\n",
        "    \n",
        "    preprocessed_frame = transform.resize(cropped_frame, [110,84])\n",
        "    return preprocessed_frame\n",
        "\n",
        "\n",
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "    frame = preprocess_frame(state)\n",
        "    if is_new_episode:\n",
        "        stacked_frames = deque([], maxlen=stack_size)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        stacked_frames.append(frame)\n",
        "        \n",
        "        \n",
        "        stacked_state = np.stack(stacked_frames,axis=2)\n",
        "    else:\n",
        "        #automatically removed oldest frame\n",
        "        stacked_frames.append(frame)\n",
        "        \n",
        "        stacked_state = np.stack(stacked_frames,axis=2)\n",
        "    \n",
        "    return stacked_state,stacked_frames\n",
        "\n",
        "def initialize_memory():\n",
        "    stacked_frames = deque([np.zeros((110,84),dtype=np.int) for i in range(stack_size)], maxlen=stack_size)\n",
        "    memory = Memory(max_size=memory_size)\n",
        "    for i in range(pretrain_length):\n",
        "        if i == 0:\n",
        "            state = env.reset()\n",
        "            state, stacked_frames = stack_frames(stacked_frames,state,True)\n",
        "        \n",
        "        randChoice = random.randint(0, len(possible_actions)-1)\n",
        "        action = possible_actions[randChoice]\n",
        "        next_state,reward,done,_ = env.step(action)\n",
        "        \n",
        "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
        "        \n",
        "        if done:\n",
        "            next_state = np.zeros(state.shape)\n",
        "            memory.add((state,action,reward,next_state,done))\n",
        "            state = env.reset()\n",
        "            state,stacked_frames = stack_frames(stacked_frames, state, True)\n",
        "        else:\n",
        "            memory.add((state,action,reward,next_state,done))\n",
        "            state=next_state\n",
        "    return memory, stacked_frames\n",
        "        \n",
        "#TODO: predict_action()\n",
        "    \n",
        "def main():\n",
        "    tf.reset_default_graph()\n",
        "    network = DQNetwork(state_size, action_size, learning_rate)\n",
        "    \n",
        "    writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
        "    tf.summary.scalar(\"Loss\", network.loss)\n",
        "    write_op = tf.summary.merge_all()\n",
        "    \n",
        "    \n",
        "    memory, stacked_frames = initialize_memory()\n",
        "    \n",
        "    #TODO: training stuff\n",
        "    \n",
        "main()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}