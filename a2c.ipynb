{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a2c.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HYw9eeaVESK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "a1dfebba-ab6d-4e21-9d48-440ea8b1053f"
      },
      "source": [
        "!apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg- dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "!apt-get install xvfb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package libav-tools is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "However the following packages replace it:\n",
            "  ffmpeg\n",
            "\n",
            "E: Unable to locate package libjpeg\n",
            "E: Unable to locate package dev\n",
            "E: Package 'libav-tools' has no installation candidate\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.2.5)\n",
            "Requirement already satisfied: piglet in /usr/local/lib/python3.6/dist-packages (0.4.4)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet) (1.6.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet) (19.1.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet) (1.1.1)\n",
            "Requirement already satisfied: Parsley in /usr/local/lib/python3.6/dist-packages (from piglet) (1.3)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet) (1.12.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet) (0.33.1)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 11 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guQ0FtblVKPX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1180
        },
        "outputId": "467b8bfa-9b9f-45e4-babe-4624a0a705d1"
      },
      "source": [
        "!pip install gym\n",
        "!pip install gym[atari]\n",
        "!pip install gym-retro\n",
        "!pip install tqdm retrowrapper gym-retro\n",
        "!pip install -U git+git://github.com/frenchie4111/dumbrain.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.16.3)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.21.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.24.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.8)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.2.1)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.16.3)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.3.0)\n",
            "Requirement already satisfied: atari-py>=0.1.4; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.1.7)\n",
            "Requirement already satisfied: PyOpenGL; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (1.24.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow; extra == \"atari\"->gym[atari]) (0.46)\n",
            "Requirement already satisfied: gym-retro in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: pyglet==1.*,>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from gym-retro) (1.3.2)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-retro) (0.10.11)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.*,>=1.3.2->gym-retro) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.16.3)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (2.21.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (1.24.2)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (2.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: retrowrapper in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: gym-retro in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: pyglet==1.*,>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from gym-retro) (1.3.2)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-retro) (0.10.11)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.*,>=1.3.2->gym-retro) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.12.0)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (2.21.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.16.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (1.24.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (2019.3.9)\n",
            "Collecting git+git://github.com/frenchie4111/dumbrain.git\n",
            "  Cloning git://github.com/frenchie4111/dumbrain.git to /tmp/pip-req-build-t2eaa14g\n",
            "  Running command git clone -q git://github.com/frenchie4111/dumbrain.git /tmp/pip-req-build-t2eaa14g\n",
            "Building wheels for collected packages: dumbrain\n",
            "  Building wheel for dumbrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-epz6msm7/wheels/50/8e/6f/47c68c95113aa8c02ac02bde75673ace7c3d3636842c75fcb6\n",
            "Successfully built dumbrain\n",
            "Installing collected packages: dumbrain\n",
            "  Found existing installation: dumbrain 0.1\n",
            "    Uninstalling dumbrain-0.1:\n",
            "      Successfully uninstalled dumbrain-0.1\n",
            "Successfully installed dumbrain-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0BlcUzpWNAf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "0cb08497-c68b-4fca-c887-16cb092cf899"
      },
      "source": [
        "!pip install baselines"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: baselines in ./baselines (0.1.5)\n",
            "Requirement already satisfied: gym<1.0.0,>=0.10.0 in /usr/local/lib/python3.6/dist-packages (from baselines) (0.10.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from baselines) (1.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from baselines) (4.28.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from baselines) (0.12.5)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from baselines) (0.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from baselines) (7.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from baselines) (3.4.5.20)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.10.0->baselines) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.10.0->baselines) (1.12.0)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.10.0->baselines) (2.21.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.10.0->baselines) (1.16.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym<1.0.0,>=0.10.0->baselines) (0.16.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym<1.0.0,>=0.10.0->baselines) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym<1.0.0,>=0.10.0->baselines) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym<1.0.0,>=0.10.0->baselines) (1.24.2)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym<1.0.0,>=0.10.0->baselines) (2.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzX8UggcVLpV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "4c423d55-0df3-44e9-ec10-b49d5685d060"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1013'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1013'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-BboFu0VOh5",
        "colab_type": "text"
      },
      "source": [
        "Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy8My5CNVQmz",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from baselines import logger\n",
        "from baselines.common.atari_wrappers import FrameStack\n",
        "\n",
        "import cv2\n",
        "\n",
        "# Custom observation wrapper to preprocess frames\n",
        "class PreprocessFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(PreprocessFrame, self).__init__(env)\n",
        "        self.width = 96\n",
        "        self.height = 96\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(self.height, self.width, 1), dtype=np.uint8)\n",
        "    \n",
        "    def observation(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "        frame = frame[:,:,None]\n",
        "        \n",
        "        return frame\n",
        "\n",
        "# Scale rewards to more reasonable amount\n",
        "class RewardScaler(gym.RewardWrapper):\n",
        "    def reward(self, reward):\n",
        "        return reward * 0.01\n",
        "        \n",
        "        \n",
        "# Create environment\n",
        "def make_env():\n",
        "    env = gym.make(\"PongDeterministic-v0\")\n",
        "    env = PreprocessFrame(env)\n",
        "    env = RewardScaler(env)\n",
        "    env = FrameStack(env, 4)\n",
        "    return env\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmyBTCAnVSLw",
        "colab_type": "text"
      },
      "source": [
        "Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekA7p664VUpw",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from baselines.common.distributions import make_pdtype\n",
        "\n",
        "def conv_layer(inputs, filters, kernel, strides, gain=1.0):\n",
        "    return tf.layers.conv2d(inputs=inputs, filters=filters, kernel_size=kernel, strides=(strides,strides), activation=tf.nn.relu, kernel_initializer=tf.orthogonal_initializer(gain=gain))\n",
        "    \n",
        "def fc_layer(inputs, nodes, activation_fn=tf.nn.relu, gain=1.0):\n",
        "    return tf.layers.dense(inputs=inputs, units=nodes, activation=activation_fn, kernel_initializer=tf.orthogonal_initializer(gain))\n",
        "    \n",
        "### Neural Network Class\n",
        "# init: constructs a convolutional network with actor and critic streams\n",
        "# step: given a state, return recommended action and value of that state\n",
        "# value: given a state, return value of that state\n",
        "# select_action: given a state, return action of that state\n",
        "\n",
        "class A2CNetwork(object):\n",
        "    def __init__(self, sess, observation_space, action_space, batches, steps, reuse = False):\n",
        "        gain = np.sqrt(2)\n",
        "        \n",
        "        self.pdtype = make_pdtype(action_space)\n",
        "        \n",
        "        height, width, channel = observation_space.shape\n",
        "        observation_shape = (height, width, channel)\n",
        "\n",
        "        inputs = tf.placeholder(tf.float32, [None, *observation_shape], name=\"input\")\n",
        "        \n",
        "        scaled_images = tf.cast(inputs, tf.float32) / 255.\n",
        "        \n",
        "        with tf.variable_scope(\"model\", reuse = reuse):\n",
        "            conv1 = conv_layer(scaled_images, 32, 8, 4, gain)\n",
        "            conv2 = conv_layer(conv1, 64, 4, 2, gain)\n",
        "            conv3 = conv_layer(conv2, 64, 3, 1, gain)\n",
        "            conv_flatten = tf.layers.flatten(conv3)\n",
        "            common_fc = fc_layer(conv_flatten, 512, gain=gain)\n",
        "            \n",
        "            # self.pi - logits, self.pd - prob distribution\n",
        "            self.pd, self.pi = self.pdtype.pdfromlatent(common_fc, init_scale=0.01)\n",
        "            \n",
        "            # value function\n",
        "            vf = fc_layer(common_fc, 1, activation_fn=None)[:, 0]\n",
        "            \n",
        "        self.initial_state = None\n",
        "        \n",
        "        a0 = self.pd.sample()\n",
        "        \n",
        "        \n",
        "        def step(state_in):\n",
        "            action, value = sess.run([a0, vf], feed_dict={inputs: state_in})\n",
        "            return action,value\n",
        "        \n",
        "        def value(state_in):\n",
        "            value = sess.run(vf, {inputs: state_in})\n",
        "            return value\n",
        "        \n",
        "        def select_action(state_in):\n",
        "            action = sess.run(a0, {inputs: state_in})\n",
        "            return action\n",
        "        \n",
        "        self.inputs = inputs\n",
        "        self.vf = vf\n",
        "        self.step = step\n",
        "        self.value = value\n",
        "        self.select_action = select_action\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sWyo19uVWEu",
        "colab_type": "text"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6IfIIBPVYWB",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "from baselines import logger\n",
        "import cv2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from baselines.a2c.utils import cat_entropy\n",
        "\n",
        "\n",
        "from baselines.common import explained_variance\n",
        "from baselines.common.runners import AbstractEnvRunner\n",
        "\n",
        "def mse(pred, target):\n",
        "    return tf.square(pred-target)/2\n",
        "\n",
        "### A2C Model Class\n",
        "# init: create two neural nets, the step and train net, and constructs loss graph\n",
        "# train: takes as input a set of states, actions, returns, and values, performs training, and outputs losses\n",
        "# save: saves model\n",
        "# load: loads model\n",
        "\n",
        "class Model():\n",
        "    \n",
        "    def __init__(self, policy, observation_space,\n",
        "    action_space, number_envs, number_steps, ent_coef, vf_coef, max_grad_norm):\n",
        "        sess = tf.get_default_session()\n",
        "        \n",
        "        actions = tf.placeholder(tf.int32, [None], name=\"actions\")\n",
        "        advantages = tf.placeholder(tf.float32, [None], name=\"advantages\")\n",
        "        rewards = tf.placeholder(tf.float32, [None], name=\"rewards\")\n",
        "        lr = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
        "        \n",
        "        step_model = policy(sess, observation_space, action_space, number_envs, 1, reuse=False)\n",
        "        \n",
        "        train_model = policy(sess, observation_space, action_space, number_envs*number_steps, number_steps, reuse=True)\n",
        "        \n",
        "        # Total Loss: policy gradient loss - (entropy * entropy_coefficient) + (value*value_coefficient)\n",
        "        \n",
        "        # -log(softmax(neural net))\n",
        "\n",
        "        neglogp = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=train_model.pi, labels=actions)\n",
        "        \n",
        "        # policy gradient loss: 1/n * sum A(s_i,a_i) * -log(softmax(neural net(a_i | s_i)))\n",
        "        pg_loss = tf.reduce_mean(advantages * neglogp)\n",
        "        \n",
        "        #value loss: 1/2 * sum (r - v(s))^2\n",
        "        \n",
        "        vf_loss = tf.reduce_mean (mse(tf.squeeze(train_model.vf), rewards))\n",
        "        \n",
        "        # improve exploration by limiting convergence\n",
        "        entropy = tf.reduce_mean(train_model.pd.entropy())\n",
        "        \n",
        "        loss = pg_loss - (entropy * ent_coef) + (vf_loss * vf_coef)\n",
        "        \n",
        "        # get trainable parameters\n",
        "        params = tf.trainable_variables(\"model\")\n",
        "        \n",
        "        # get gradient\n",
        "        grads = tf.gradients(loss, params)\n",
        "        if max_grad_norm is not None:\n",
        "            grads, grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)\n",
        "        grads = list(zip(grads,params))\n",
        "        \n",
        "        trainer = tf.train.RMSPropOptimizer(lr, decay=0.99, epsilon=1e-5)\n",
        "        \n",
        "        train_obj = trainer.apply_gradients(grads)\n",
        "        \n",
        "        # Calculate advantage from returns and values, use to compute\n",
        "        def train(states_in, actions_, returns_, values_, lr_):\n",
        "            \n",
        "            # returns = (bootstrap) q-value estimation (reward + gamma*V(s'))\n",
        "            advantages_ = returns_ - values_\n",
        "            \n",
        "            feed_dict = {train_model.inputs: states_in,\n",
        "            actions: actions_,\n",
        "            advantages: advantages_,\n",
        "            rewards: returns_,\n",
        "            lr: lr_}\n",
        "            \n",
        "            policy_loss, value_loss, entropy_, _ = sess.run([pg_loss, vf_loss, entropy, train_obj], feed_dict=feed_dict)\n",
        "            \n",
        "            return policy_loss, value_loss, entropy_\n",
        "        \n",
        "        def save(save_path):\n",
        "            saver = tf.train.Saver()\n",
        "            saver.save(sess, save_path)\n",
        "            \n",
        "        def load(save_path):\n",
        "            saver = tf.train.Saver()\n",
        "            saver.restore(sess, save_path)\n",
        "            print(\"Model loaded.\")\n",
        "            \n",
        "        \n",
        "        self.train = train\n",
        "        self.train_model = train_model\n",
        "        self.save = save\n",
        "        self.load = load\n",
        "        self.step = step_model.step\n",
        "        self.step_model = step_model\n",
        "        self.value = step_model.value\n",
        "        self.initial_state = step_model.initial_state\n",
        "        \n",
        "        tf.global_variables_initializer().run(session=sess)\n",
        "\n",
        "# Runs training\n",
        "class Runner(AbstractEnvRunner):\n",
        "    def __init__(self, env, model, number_steps, total_timesteps, gamma, lam):\n",
        "        super().__init__(env=env, model=model, nsteps=number_steps)\n",
        "        \n",
        "        self.gamma = gamma\n",
        "        \n",
        "        self.lam = lam\n",
        "        \n",
        "        self.total_timesteps = total_timesteps\n",
        "        \n",
        "    # Collect a set of experiences\n",
        "    def run(self):\n",
        "        observations_list, actions_list, rewards_list, values_list, dones_list = [], [], [], [], []\n",
        "        \n",
        "        for n in range(self.nsteps):\n",
        "            actions,values = self.model.step(self.obs)\n",
        "            \n",
        "            observations_list.append(np.copy(self.obs))\n",
        "            actions_list.append(actions)\n",
        "            values_list.append(values)\n",
        "            dones_list.append(np.copy(self.dones))\n",
        "            \n",
        "            self.obs[:], rewards, self.dones, _ = self.env.step(actions)\n",
        "            \n",
        "            rewards_list.append(rewards)\n",
        "        \n",
        "        observations_list = np.asarray(observations_list, dtype=np.uint8)\n",
        "        actions_list = np.asarray(actions_list, dtype=np.int32)\n",
        "        rewards_list = np.asarray(rewards_list, dtype=np.float32)\n",
        "        values_list = np.asarray(values_list, dtype=np.float32)\n",
        "        dones_list = np.asarray(dones_list, dtype=np.bool)\n",
        "        \n",
        "        last_values = self.model.value(self.obs)\n",
        "        \n",
        "        returns_list = np.zeros_like(rewards_list)\n",
        "        advantages_list = np.zeros_like(rewards_list)\n",
        "        \n",
        "        last_gae_lam = 0\n",
        "        \n",
        "        for t in reversed(range(self.nsteps)):\n",
        "            \n",
        "            # if we are in a final state, there is no value of the next state, so set modifier (nextnonterminal) to 0\n",
        "            if t == self.nsteps - 1:\n",
        "                next_non_terminal = 1.0 - self.dones\n",
        "                next_values = last_values\n",
        "            else:\n",
        "                next_non_terminal = 1.0 - dones_list[t+1]\n",
        "                next_values = values_list[t+1]\n",
        "            \n",
        "            # return function. r_t + gamma*V(s_t+1) - V(s_t)\n",
        "            delta = rewards_list[t] + self.gamma * next_values * next_non_terminal - values_list[t]\n",
        "            \n",
        "            # advantage. delta + gamma * lambda * next_non_terminal * last_gae_lam\n",
        "            advantages_list[t] = delta + self.gamma * self.lam * next_non_terminal * last_gae_lam\n",
        "            last_gae_lam = advantages_list[t]\n",
        "        \n",
        "        returns_list = advantages_list + values_list\n",
        "        \n",
        "        return map(sf01, (observations_list, actions_list, returns_list, values_list, rewards_list))\n",
        "        \n",
        "def sf01(arr):\n",
        "    s = arr.shape\n",
        "    swapped = arr.swapaxes(0,1)\n",
        "    reshaped = swapped.reshape(s[0] * s[1], *s[2:])\n",
        "    return reshaped\n",
        "    \n",
        "## Learn Function\n",
        "# Takes as input a policy, environment, number of steps, total number of time steps, gamma, lambda, value coefficient, entropy coefficient, learning rate, max gradient norm, log_interval and executes training\n",
        "def learn(policy, env, nsteps, total_timesteps, gamma, lam, vf_coef, ent_coef, lr, max_grad_norm, log_interval, restart):\n",
        "    number_epochs = 4\n",
        "    number_mini_batches = 8\n",
        "    \n",
        "    nenvs = env.num_envs\n",
        "    \n",
        "    observation_space = env.observation_space\n",
        "    action_space = env.action_space\n",
        "    \n",
        "    batch_size = nenvs * nsteps\n",
        "    batch_train_size = batch_size // number_mini_batches\n",
        "    \n",
        "    assert batch_size % number_mini_batches == 0\n",
        "    \n",
        "    # Construct model\n",
        "    model = Model(policy=policy,\n",
        "                observation_space=observation_space,\n",
        "                action_space=action_space,\n",
        "                number_envs=nenvs,\n",
        "                number_steps=nsteps,\n",
        "                ent_coef=ent_coef,\n",
        "                vf_coef=vf_coef,\n",
        "                max_grad_norm=max_grad_norm\n",
        "                )\n",
        "\n",
        "    if not restart:\n",
        "        model.load(\"./models/model.ckpt\")\n",
        "\n",
        "    runner = Runner(env, model, nsteps, total_timesteps, gamma, lam)\n",
        "    \n",
        "    time_first_start = time.time()\n",
        "    \n",
        "    # For every batch of the game\n",
        "    for update in range(1, total_timesteps // batch_size+1):\n",
        "        time_start = time.time()\n",
        "        \n",
        "        # Generate observations\n",
        "        observations, actions, returns, values, rewards = runner.run()\n",
        "        \n",
        "        reward_sum = np.sum(rewards)\n",
        "        \n",
        "        losses_list = []\n",
        "        total_batches_train = 0\n",
        "        \n",
        "        indices = np.arange(batch_size)\n",
        "        for epoch in range(number_epochs):\n",
        "            np.random.shuffle(indices)\n",
        "            \n",
        "            # Feed minibatches to model for training\n",
        "            for mini_start in range(0, batch_size, batch_train_size):\n",
        "                mini_end = mini_start + batch_train_size\n",
        "                mini_indices = indices[mini_start:mini_end]\n",
        "\n",
        "                mini_states = observations[mini_indices]\n",
        "                mini_actions = actions[mini_indices]\n",
        "                mini_returns = returns[mini_indices]\n",
        "                mini_values = values[mini_indices]\n",
        "\n",
        "                losses_list.append(model.train(mini_states,mini_actions,mini_returns,mini_values,lr))\n",
        "        \n",
        "        loss_values = np.mean(losses_list, axis=0)\n",
        "        time_now = time.time()\n",
        "        fps = int(batch_size / (time_now - time_start))\n",
        "        \n",
        "        # Print out updates\n",
        "        if update % log_interval == 0 or update == 1:\n",
        "            ev = explained_variance(values, returns)\n",
        "            logger.record_tabular(\"nupdates\", update)\n",
        "            logger.record_tabular(\"total_timesteps\", update*batch_size)\n",
        "            logger.record_tabular(\"fps\", fps)\n",
        "            logger.record_tabular(\"policy_loss\", float(loss_values[0]))\n",
        "            logger.record_tabular(\"policy_entropy\", float(loss_values[2]))\n",
        "            logger.record_tabular(\"value_loss\", float(loss_values[1]))\n",
        "            logger.record_tabular(\"explained_variance\", float(ev))\n",
        "            logger.record_tabular(\"time_elapsed\", float(time_now - time_first_start))\n",
        "            logger.record_tabular(\"total_reward\", float(reward_sum))\n",
        "            logger.dump_tabular()\n",
        "\n",
        "            model.save(\"./models/model.ckpt\")\n",
        "            print(\"Model saved.\")\n",
        "    env.close()\n",
        "\n",
        "def play(policy, env):\n",
        "    observation_space = env.observation_space\n",
        "    action_space = env.action_space\n",
        "    \n",
        "    model = Model(policy=policy,\n",
        "                  observation_space=observation_space,\n",
        "                  action_space=action_space,\n",
        "                  number_envs=1,\n",
        "                  number_steps=1,\n",
        "                  ent_coef=0,\n",
        "                  vf_coef=0,\n",
        "                  max_grad_norm=0)\n",
        "                  \n",
        "    model.load(\"./models/model.ckpt\")\n",
        "    \n",
        "    obs = env.reset()\n",
        "    \n",
        "    score=0\n",
        "    done = False\n",
        "    while done == False:\n",
        "        actions,values = model.step(obs)\n",
        "        obs, rewards,done,_ = env.step(actions)\n",
        "        score += rewards\n",
        "        \n",
        "        env.render()\n",
        "    \n",
        "    print(\"Score: \", score)\n",
        "    env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx5SNtTJVaH6",
        "colab_type": "text"
      },
      "source": [
        "Agent Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGty1bIOVZtT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1472
        },
        "outputId": "a65b0c1a-22c0-4b5c-d468-5621a5e13a81"
      },
      "source": [
        "import math\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
        "\n",
        "def main():\n",
        "    config = tf.ConfigProto()\n",
        "    \n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    \n",
        "    config.gpu_options.allow_growth = True\n",
        "    environment_list = []\n",
        "    for i in range(10):\n",
        "        environment_list.append(make_env)\n",
        "    \n",
        "    env_vector = SubprocVecEnv(environment_list)\n",
        "    \n",
        "    with tf.Session(config=config):\n",
        "        learn(policy=A2CNetwork,\n",
        "                    env=env_vector,\n",
        "                    nsteps=2048,\n",
        "                    total_timesteps=10000000,\n",
        "                    gamma=0.99,\n",
        "                    lam=0.95,\n",
        "                    vf_coef=0.5,\n",
        "                    ent_coef=0.01,\n",
        "                    lr=2e-4,\n",
        "                    max_grad_norm=0.5,\n",
        "                    log_interval=10,\n",
        "                    restart=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-37b22a40075e>:7: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-2-37b22a40075e>:35: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-2-37b22a40075e>:10: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Logging to /tmp/openai-2019-05-09-02-53-44-418423\n",
            "---------------------------------\n",
            "| explained_variance | -0.422   |\n",
            "| fps                | 519      |\n",
            "| nupdates           | 1        |\n",
            "| policy_entropy     | 1.79     |\n",
            "| policy_loss        | 0.0958   |\n",
            "| time_elapsed       | 39.4     |\n",
            "| total_reward       | -4.35    |\n",
            "| total_timesteps    | 2.05e+04 |\n",
            "| value_loss         | 0.000716 |\n",
            "---------------------------------\n",
            "Model saved.\n",
            "---------------------------------\n",
            "| explained_variance | -2.81    |\n",
            "| fps                | 594      |\n",
            "| nupdates           | 10       |\n",
            "| policy_entropy     | 1.79     |\n",
            "| policy_loss        | 0.0168   |\n",
            "| time_elapsed       | 361      |\n",
            "| total_reward       | -4.37    |\n",
            "| total_timesteps    | 2.05e+05 |\n",
            "| value_loss         | 9.56e-05 |\n",
            "---------------------------------\n",
            "Model saved.\n",
            "---------------------------------\n",
            "| explained_variance | -4.11    |\n",
            "| fps                | 596      |\n",
            "| nupdates           | 20       |\n",
            "| policy_entropy     | 1.79     |\n",
            "| policy_loss        | 0.00276  |\n",
            "| time_elapsed       | 709      |\n",
            "| total_reward       | -4.46    |\n",
            "| total_timesteps    | 4.1e+05  |\n",
            "| value_loss         | 7.53e-05 |\n",
            "---------------------------------\n",
            "Model saved.\n",
            "---------------------------------\n",
            "| explained_variance | -0.0893  |\n",
            "| fps                | 584      |\n",
            "| nupdates           | 30       |\n",
            "| policy_entropy     | 1.79     |\n",
            "| policy_loss        | -0.08    |\n",
            "| time_elapsed       | 1.06e+03 |\n",
            "| total_reward       | -4.28    |\n",
            "| total_timesteps    | 6.14e+05 |\n",
            "| value_loss         | 0.00765  |\n",
            "---------------------------------\n",
            "Model saved.\n",
            "---------------------------------\n",
            "| explained_variance | -0.0843  |\n",
            "| fps                | 593      |\n",
            "| nupdates           | 40       |\n",
            "| policy_entropy     | 1.79     |\n",
            "| policy_loss        | -0.0634  |\n",
            "| time_elapsed       | 1.4e+03  |\n",
            "| total_reward       | -4.53    |\n",
            "| total_timesteps    | 8.19e+05 |\n",
            "| value_loss         | 0.000397 |\n",
            "---------------------------------\n",
            "Model saved.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbb7RnfQcenM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls\n",
        "!zip -r models.zip models/\n",
        "from google.colab import files\n",
        "files.download(\"./models.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}